{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":83173,"databundleVersionId":9700231,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', None)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom collections import defaultdict\nimport random\nfrom torch.utils.data import Dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-12T00:19:04.689413Z","iopub.execute_input":"2025-01-12T00:19:04.689811Z","iopub.status.idle":"2025-01-12T00:19:07.630323Z","shell.execute_reply.started":"2025-01-12T00:19:04.689775Z","shell.execute_reply":"2025-01-12T00:19:07.629119Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_dataframes = []\n#for i in tqdm(range(1, 11)):\n    #train_dataframes.append(pd.read_csv(f'/kaggle/input/product-re-purchase-prediction/data-train/data-train/train_data_part_{i}.csv'))\ntrain_dataframes.append(pd.read_csv(f'/kaggle/input/product-re-purchase-prediction/data-train/data-train/train_data_part_1.csv'))\ntrain_data = pd.concat(train_dataframes, ignore_index=True)\n\ndel train_dataframes\n\nproducts_data = pd.read_csv('/kaggle/input/product-re-purchase-prediction/data-train/data-train/products_data.csv', low_memory=False)\ntest_data = pd.read_csv('/kaggle/input/product-re-purchase-prediction/data-train/data-train/test_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T22:22:07.944310Z","iopub.execute_input":"2025-01-11T22:22:07.944668Z","iopub.status.idle":"2025-01-11T22:22:29.953795Z","shell.execute_reply.started":"2025-01-11T22:22:07.944629Z","shell.execute_reply":"2025-01-11T22:22:29.952801Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_data['date'] = pd.to_datetime(train_data['date'])\n\n# Add recency attribute\nlatest_date = train_data['date'].max()  # Find the latest date in the dataset\ntrain_data['recency'] = (latest_date - train_data['date']).dt.days  # Calculate days since last purchase\n\n# Group by customer_id and product_id to calculate quantity and most recent purchase\ncustomer_product_data = train_data.groupby(['customer_id', 'product_id']).agg({\n    'quantity': 'sum',\n    'recency': 'min'  # Minimum days since purchase (most recent)\n}).reset_index()\n\n# Normalize quantity and recency scores\ncustomer_product_data['quantity_score'] = customer_product_data['quantity'] / customer_product_data['quantity'].max()\ncustomer_product_data['recency_score'] = 1 - (customer_product_data['recency'] / customer_product_data['recency'].max())  # Recent = higher score\n\n# Merge frequency data into customer_product_data\nfrequency = train_data.groupby(['customer_id', 'product_id'])['transaction_id'].count().reset_index(name='frequency')\ncustomer_product_data = customer_product_data.merge(frequency, on=['customer_id', 'product_id'], how='left')\n\n# Fill any missing frequency values (if any product has no transactions counted, assume 0)\ncustomer_product_data['frequency'] = customer_product_data['frequency'].fillna(0)\n\n# Normalize frequency score\ncustomer_product_data['frequency_score'] = customer_product_data['frequency'] / customer_product_data['frequency'].max()\n\n# Compute final score\n# Define the set of popular items\npopular_items = {'Product_23971', 'Product_28633', 'Product_39751', 'Product_20421', 'Product_63301', 'Product_57942'}\n\n# Add a column to indicate if a product is popular\ncustomer_product_data['is_popular'] = customer_product_data['product_id'].isin(popular_items).astype(int)\n\nBest_alpha=0.03\nBest_beta=0.87\nBest_gamma=0.1\nBest_leverage=0.0019395677472984205\n\n# Recalculate the final score, adding leverage for popular items\ncustomer_product_data['final_score'] = (\n    Best_alpha * customer_product_data['quantity_score'] + \n    Best_beta * customer_product_data['frequency_score'] +\n    Best_gamma * customer_product_data['recency_score'] +\n    Best_leverage * customer_product_data['is_popular']  # Add leverage\n)\n\n# Merge final score back into train_data\ntrain_data = train_data.merge(\n    customer_product_data[['customer_id', 'product_id', 'final_score']],\n    on=['customer_id', 'product_id'],\n    how='left'\n)\n\n# Merge train_data with products_data\n#data_train = pd.merge(train_data, products_data, on=\"product_id\", how=\"left\")\ndata_train = train_data.copy()\n\n# Sort data_train by date\ndata_train = data_train.sort_values(by='date')\ndata_train.reset_index(drop=True, inplace=True)\n\n# Drop unnecessary columns\ndata_train = data_train.drop(columns=['date'])\n\n# Move final_score to the last column\nfinal_score_column = data_train.pop('final_score')  # Remove `final_score` and store it\ndata_train['final_score'] = final_score_column     # Reinsert `final_score` at the end\n\nprint(data_train.head(1))\nprint(\"Finish Data Preparation!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T22:22:29.955963Z","iopub.execute_input":"2025-01-11T22:22:29.956312Z","iopub.status.idle":"2025-01-11T22:23:07.833053Z","shell.execute_reply.started":"2025-01-11T22:22:29.956279Z","shell.execute_reply":"2025-01-11T22:23:07.831925Z"}},"outputs":[{"name":"stdout","text":"        transaction_id     customer_id     product_id  has_loyality_card  \\\n0  Transaction_2754179  Household_8373  Product_73736                  0   \n\n    store_id  is_promo  quantity format order_channel  recency  final_score  \n0  Store_552         0       1.0  DRIVE       WEBSITE      729     0.109268  \nFinish Data Preparation!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Step 1: Split User-Item Sequences for Train and Validation\nuser_item_sequences = defaultdict(list)\n\n# Construct sequences for each user\nfor _, row in data_train.iterrows():\n    user_item_sequences[row['customer_id']].append(row['product_id'])\n\n# Convert to a dictionary for easy access\nuser_item_sequences = dict(user_item_sequences)\n\n# Step 2: Split users into train and validation sets\ntrain_sequences = {}\nval_sequences = {}\n\n# Split users into train and validation sets (80% training, 20% validation)\nall_users = list(user_item_sequences.keys())\nrandom.shuffle(all_users)\n\nval_split_ratio = 0.2\nval_user_count = int(len(all_users) * val_split_ratio)\nval_users = all_users[:val_user_count]\ntrain_users = all_users[val_user_count:]\n\n# Create separate sequences for train and validation\nfor user in train_users:\n    train_sequences[user] = user_item_sequences[user][:-1]  # All except last item for training\nfor user in val_users:\n    val_sequences[user] = user_item_sequences[user][-1:]  # The last item for validation\n\n# Step 3: Encode Users and Items\nuser_encoder = {user: idx for idx, user in enumerate(user_item_sequences.keys())}\nitem_encoder = {item: idx for idx, item in enumerate(set(data_train['product_id']))}\n\n# Step 4: Prepare Sequences for Train and Validation\ndef encode_sequences(sequences, max_seq_length):\n    encoded_sequences = {\n        user_encoder[user]: [item_encoder[item] for item in items]\n        for user, items in sequences.items()\n    }\n\n    padded_sequences = []\n    targets = []\n    users = []\n\n    for user, seq in encoded_sequences.items():\n        if len(seq) >= max_seq_length:\n            seq = seq[-max_seq_length:]  # Keep only the last `max_seq_length` items\n        else:\n            seq = [0] * (max_seq_length - len(seq)) + seq  # Pad with zeros\n        \n        padded_sequences.append(seq)\n        targets.append(seq[-1])  # Predict the last item in the sequence\n        users.append(user)\n\n    return torch.tensor(padded_sequences, dtype=torch.long), torch.tensor(targets, dtype=torch.long), torch.tensor(users, dtype=torch.long)\n\nmax_seq_length = 10\ntrain_padded_sequences, train_targets, train_users = encode_sequences(train_sequences, max_seq_length)\nval_padded_sequences, val_targets, val_users = encode_sequences(val_sequences, max_seq_length)\n\n# Step 5: Create DataLoaders for Train and Validation Sets\ntrain_dataset = TensorDataset(train_users, train_padded_sequences, train_targets)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\nval_dataset = TensorDataset(val_users, val_padded_sequences, val_targets)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n\n# Hyperparameters\nnum_items = len(item_encoder)\nembedding_dim = 50\nmax_seq_length = train_padded_sequences.size(1)\nnum_h_filters = 16\nnum_v_filters = 8\nhorizontal_filter_sizes = [2, 3, 4]\nnum_negatives = 30\nbatch_size = 64\nepochs = 10\nlearning_rate = 0.001\nweight_decay = 1e-4\n\n# Load product features from products_data\nproduct_features = data_train.drop(columns=['transaction_id',\t'customer_id',\t'product_id', 'store_id', 'format',\t'order_channel']).values  # Drop non-feature columns\n\n# Ensure product IDs are encoded\nproduct_encoder = {item: idx for idx, item in enumerate(products_data['product_id'].unique())}\nencoded_product_ids = [product_encoder[pid] for pid in train_data['product_id']]\n\n# Align features with product IDs\nitem_features_tensor = torch.zeros(len(product_encoder), product_features.shape[1])\nfor pid, features in zip(encoded_product_ids, product_features):\n    item_features_tensor[pid] = torch.tensor(features)\n\nfeature_dim = item_features_tensor.size(1)  # Get feature dimension\n\n# Ensure consistent user and item encoding\nnum_users = len(user_encoder)\nnum_items = len(item_encoder)\n                \nprint(\"Finish Data Preparation!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T22:23:07.835131Z","iopub.execute_input":"2025-01-11T22:23:07.835591Z","iopub.status.idle":"2025-01-11T22:32:24.210325Z","shell.execute_reply.started":"2025-01-11T22:23:07.835543Z","shell.execute_reply":"2025-01-11T22:32:24.208861Z"}},"outputs":[{"name":"stdout","text":"Finish Data Preparation!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Candidate generation function\ndef get_candidates(user_seq, candidate_size=50):\n    global_top_items = torch.arange(num_items)[:candidate_size]\n    return global_top_items\n\nclass CaserDatasetWithNegatives(Dataset):\n    def __init__(self, sequences, targets, num_items, num_negatives=1):\n        self.sequences = sequences\n        self.targets = targets\n        self.num_items = num_items\n        self.num_negatives = num_negatives\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        sequence = self.sequences[idx]\n        target = self.targets[idx]\n\n        # Generate negative samples\n        negatives = []\n        while len(negatives) < self.num_negatives:\n            neg = random.randint(0, self.num_items - 1)\n            if neg != target:  # Ensure it's not the positive item\n                negatives.append(neg)\n\n        # Ensure everything is returned as a tensor\n        return {\n            \"user_id\": torch.tensor(idx, dtype=torch.long),\n            \"sequence\": torch.tensor(sequence, dtype=torch.long),\n            \"target\": torch.tensor(target, dtype=torch.long),\n            \"negatives\": torch.tensor(negatives, dtype=torch.long),\n        }\n\n# Create DataLoader for the training set with negative samples\ntrain_dataset = CaserDatasetWithNegatives(\n    sequences=train_padded_sequences,\n    targets=train_targets,\n    num_items=num_items,\n    num_negatives=num_negatives,\n)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Similarly, create DataLoader for the validation set\nval_dataset = CaserDatasetWithNegatives(\n    sequences=val_padded_sequences,\n    targets=val_targets,\n    num_items=num_items,\n    num_negatives=num_negatives,\n)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nprint(\"Finish Data Preprocessing!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T22:32:24.212023Z","iopub.execute_input":"2025-01-11T22:32:24.213087Z","iopub.status.idle":"2025-01-11T22:32:24.289924Z","shell.execute_reply.started":"2025-01-11T22:32:24.213048Z","shell.execute_reply":"2025-01-11T22:32:24.288538Z"}},"outputs":[{"name":"stdout","text":"Finish Data Preprocessing!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Caser Model\nclass Caser(nn.Module):\n    def __init__(self, num_items, embedding_dim, num_h_filters, num_v_filters):\n        super(Caser, self).__init__()\n        self.num_items = num_items\n        self.embedding_dim = embedding_dim\n        self.num_h_filters = num_h_filters\n        self.num_v_filters = num_v_filters\n\n        # Embedding layers for items\n        self.item_embeddings = nn.Embedding(num_items, embedding_dim)\n\n        # Horizontal and vertical convolutional layers\n        self.h_conv = nn.Conv1d(in_channels=embedding_dim, out_channels=num_h_filters, kernel_size=3, padding=1)\n        self.v_conv = nn.Conv1d(in_channels=embedding_dim, out_channels=num_v_filters, kernel_size=3, padding=1)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(num_h_filters + num_v_filters, 50)  # Adjust the input size based on combined output\n        self.fc2 = nn.Linear(50, num_items)\n        self.dropout = nn.Dropout(p=0.5)\n\n    def forward(self, user_ids, item_seq, item_features):\n        item_embeds = self.item_embeddings(item_seq)  # Shape: (batch_size, seq_len, embedding_dim)\n\n        # Horizontal and vertical convolutions\n        h_out = self.h_conv(item_embeds.transpose(1, 2))  # Shape: (batch_size, num_h_filters, seq_len)\n        v_out = self.v_conv(item_embeds.transpose(1, 2))  # Shape: (batch_size, num_v_filters, seq_len)\n\n        # Apply dropout to convolution outputs\n        h_out = self.dropout(h_out)\n        v_out = self.dropout(v_out)\n\n        \n        # Global average pooling for each output\n        h_out = h_out.mean(dim=2)  # Shape: (batch_size, num_h_filters)\n        v_out = v_out.mean(dim=2)  # Shape: (batch_size, num_v_filters)\n\n        # Concatenate the outputs from the two convolutions\n        combined_output = torch.cat([h_out, v_out], dim=1)  # Shape: (batch_size, num_h_filters + num_v_filters)\n\n        # Fully connected layers\n        x = torch.relu(self.fc1(combined_output))  # Shape: (batch_size, 50)\n        x = self.dropout(x)\n        scores = self.fc2(x)  # Shape: (batch_size, 1)\n\n        return scores\n\n# Instantiate Model\nmodel = Caser(\n    num_items=num_items,                  # Total number of items\n    embedding_dim=embedding_dim,         # Dimensionality of embeddings\n    num_h_filters=num_h_filters,         # Number of horizontal filters\n    num_v_filters=num_v_filters,         # Number of vertical filters\n).to(device)\n\n\n# Loss and Optimizer\ncriterion = nn.BCEWithLogitsLoss()  # For binary classification between positive and negatives\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# Step 6: Adjust the Model Training Loop\ntrain_losses=[]\nval_losses=[]\nhitrate10_scores=[]\n\nepochs = 30\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0\n\n    # Training loop\n    for batch in train_loader:\n        # Unpack dictionary from the dataset\n        user_ids = batch['sequence']\n        sequences = batch['sequence']\n        targets = batch['target']\n        negatives = batch[\"negatives\"].to(device)  # Shape: (batch_size, num_negatives)\n    \n        user_ids, sequences, targets = user_ids.to(device), sequences.to(device), targets.to(device)\n    \n        optimizer.zero_grad()\n    \n        # Forward pass for positive items\n        positive_scores = model(user_ids, sequences, item_features_tensor.to(device))  # Shape: (batch_size, num_items)\n        positive_scores = torch.gather(positive_scores, 1, targets.unsqueeze(1)).squeeze(1)  # Shape: (batch_size,)\n        \n        # Negative sampling\n        negative_scores = model(user_ids, sequences, item_features_tensor.to(device))  # Shape: (batch_size, num_items)\n        negative_scores = torch.gather(negative_scores, 1, negatives)  # Shape: (batch_size, num_negatives)\n        \n        # Reshape positive_scores for concatenation\n        positive_scores = positive_scores.unsqueeze(1)  # Shape: (batch_size, 1)\n        \n        # Concatenate scores and labels\n        all_scores = torch.cat([positive_scores, negative_scores], dim=1)  # Shape: (batch_size, 1 + num_negatives)\n        pos_labels = torch.ones_like(positive_scores)  # Shape: (batch_size, 1)\n        neg_labels = torch.zeros_like(negative_scores)  # Shape: (batch_size, num_negatives)\n        all_labels = torch.cat([pos_labels, neg_labels], dim=1)  # Shape: (batch_size, 1 + num_negatives)\n        \n        # Flatten for loss calculation\n        loss = criterion(all_scores.view(-1), all_labels.view(-1))\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n    train_losses.append(train_loss)\n    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}\")\n\n    # Validation loop\n    model.eval()\n    val_loss = 0\n    hitrate = 0\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            user_ids = batch[\"user_id\"].to(device)\n            sequences = batch[\"sequence\"].to(device)\n            targets = batch[\"target\"].to(device)\n            negatives = batch[\"negatives\"].to(device)  # Should be shape (batch_size, num_negatives)\n        \n            # Forward pass for positive items\n            positive_scores = model(user_ids, sequences, item_features_tensor.to(device))  # Shape: (batch_size, num_items)\n        \n            # Reshape positive_scores for negative sampling\n            positive_scores = torch.gather(positive_scores, 1, targets.unsqueeze(1))  # Shape: (batch_size,)\n            \n            # Negative sampling - same as positive scoring\n            negative_scores = model(user_ids, sequences, item_features_tensor.to(device))  # Shape: (batch_size, num_items)\n            negative_scores = torch.gather(negative_scores, 1, negatives)  # Shape: (batch_size, num_negatives)\n            \n            # Concatenate scores and labels\n            all_scores = torch.cat([positive_scores, negative_scores], dim=1)  # Shape: (batch_size, 1 + num_negatives)\n            \n            pos_labels = torch.ones_like(positive_scores)  # Shape: (batch_size, 1)\n            neg_labels = torch.zeros_like(negative_scores)  # Shape: (batch_size, num_negatives)\n            all_labels = torch.cat([pos_labels, neg_labels], dim=1)  # Shape: (batch_size, 1 + num_negatives)\n            \n            # Flatten for BCEWithLogitsLoss\n            loss = criterion(all_scores.view(-1), all_labels.view(-1))  # Flatten both tensors\n        \n            val_loss += loss.item()\n        \n            # Calculate HR@10 for the batch\n            hitrate_batch = 0\n            for i in range(targets.size(0)):\n                if targets[i] in negatives[i]:\n                    hitrate_batch += 1\n            hitrate += hitrate_batch / targets.size(0)\n\n    val_loss /= len(val_loader)\n    val_losses.append(val_loss)\n    \n    hitrate10_scores.append(hitrate / len(val_loader))\n    \n    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T22:32:24.292009Z","iopub.execute_input":"2025-01-11T22:32:24.292495Z","iopub.status.idle":"2025-01-11T22:38:30.812520Z","shell.execute_reply.started":"2025-01-11T22:32:24.292447Z","shell.execute_reply":"2025-01-11T22:38:30.811315Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_114/2752986498.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  \"sequence\": torch.tensor(sequence, dtype=torch.long),\n/tmp/ipykernel_114/2752986498.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  \"target\": torch.tensor(target, dtype=torch.long),\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30, Train Loss: 0.6834\nEpoch 1/30, Validation Loss: 0.6433\nEpoch 2/30, Train Loss: 0.5134\nEpoch 2/30, Validation Loss: 0.2240\nEpoch 3/30, Train Loss: 0.2203\nEpoch 3/30, Validation Loss: 0.1331\nEpoch 4/30, Train Loss: 0.1363\nEpoch 4/30, Validation Loss: 0.1223\nEpoch 5/30, Train Loss: 0.1122\nEpoch 5/30, Validation Loss: 0.1220\nEpoch 6/30, Train Loss: 0.1019\nEpoch 6/30, Validation Loss: 0.1181\nEpoch 7/30, Train Loss: 0.0957\nEpoch 7/30, Validation Loss: 0.1204\nEpoch 8/30, Train Loss: 0.0922\nEpoch 8/30, Validation Loss: 0.1260\nEpoch 9/30, Train Loss: 0.0889\nEpoch 9/30, Validation Loss: 0.1277\nEpoch 10/30, Train Loss: 0.0861\nEpoch 10/30, Validation Loss: 0.1337\nEpoch 11/30, Train Loss: 0.0842\nEpoch 11/30, Validation Loss: 0.1309\nEpoch 12/30, Train Loss: 0.0833\nEpoch 12/30, Validation Loss: 0.1292\nEpoch 13/30, Train Loss: 0.0819\nEpoch 13/30, Validation Loss: 0.1384\nEpoch 14/30, Train Loss: 0.0794\nEpoch 14/30, Validation Loss: 0.1374\nEpoch 15/30, Train Loss: 0.0787\nEpoch 15/30, Validation Loss: 0.1364\nEpoch 16/30, Train Loss: 0.0778\nEpoch 16/30, Validation Loss: 0.1253\nEpoch 17/30, Train Loss: 0.0773\nEpoch 17/30, Validation Loss: 0.1406\nEpoch 18/30, Train Loss: 0.0764\nEpoch 18/30, Validation Loss: 0.1353\nEpoch 19/30, Train Loss: 0.0750\nEpoch 19/30, Validation Loss: 0.1483\nEpoch 20/30, Train Loss: 0.0733\nEpoch 20/30, Validation Loss: 0.1444\nEpoch 21/30, Train Loss: 0.0718\nEpoch 21/30, Validation Loss: 0.1361\nEpoch 22/30, Train Loss: 0.0712\nEpoch 22/30, Validation Loss: 0.1357\nEpoch 23/30, Train Loss: 0.0693\nEpoch 23/30, Validation Loss: 0.1500\nEpoch 24/30, Train Loss: 0.0685\nEpoch 24/30, Validation Loss: 0.1485\nEpoch 25/30, Train Loss: 0.0664\nEpoch 25/30, Validation Loss: 0.1520\nEpoch 26/30, Train Loss: 0.0642\nEpoch 26/30, Validation Loss: 0.1661\nEpoch 27/30, Train Loss: 0.0633\nEpoch 27/30, Validation Loss: 0.1645\nEpoch 28/30, Train Loss: 0.0611\nEpoch 28/30, Validation Loss: 0.1728\nEpoch 29/30, Train Loss: 0.0591\nEpoch 29/30, Validation Loss: 0.1755\nEpoch 30/30, Train Loss: 0.0577\nEpoch 30/30, Validation Loss: 0.1930\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"user_item_history = train_data.groupby('customer_id')['product_id'].apply(list).to_dict()\n\ndef create_product_id_mapping(user_item_history):\n    \"\"\"\n    Create a mapping from product_id (string) to an integer.\n    \"\"\"\n    unique_product_ids = set([product_id for seq in user_item_history.values() for product_id in seq])\n    product_id_mapping = {product_id: idx for idx, product_id in enumerate(unique_product_ids)}\n    return product_id_mapping\n\ndef pad_sequences(user_item_history, num_items, product_id_mapping, device='cpu'):\n    # Pad sequences to a minimum length and convert product_ids to integers\n    padded_sequences = {}\n    for user_id, seq in user_item_history.items():\n        # Convert product IDs to integers using the mapping\n        seq = [product_id_mapping.get(product_id, -1) for product_id in seq]  # -1 for unknown product_ids\n\n        if len(seq) < num_items:  # Pad if sequence length is smaller\n            pad_length = num_items - len(seq)\n            seq = seq + [0] * pad_length  # Padding with 0\n        padded_sequences[user_id] = torch.tensor(seq, dtype=torch.long).to(device)\n\n    return padded_sequences\n\n# Create product ID mapping\nproduct_id_mapping = create_product_id_mapping(user_item_history)\n\n# Apply padding to user-item history with the mapping\npadded_user_item_history = pad_sequences(user_item_history, num_items, product_id_mapping, device='cpu')\n\ndef generate_top_10_recommendations(model, train_data, item_features, num_users, num_items, product_id_mapping, device='cpu'):\n    \"\"\"\n    Generate top 10 recommendations for each user from the trained Caser model.\n\n    Args:\n        model: Trained Caser model.\n        train_data: DataFrame containing historical transaction data.\n        item_features: Tensor containing product features.\n        num_users: Total number of users in the dataset.\n        num_items: Total number of items in the dataset.\n        product_id_mapping: Mapping from product_id (str) to integer index.\n        device: Device to run the model on (e.g., 'cpu' or 'cuda').\n\n    Returns:\n        DataFrame with columns: 'customer_id', 'product_id', 'rank'.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n\n    # Create user-item history from transaction data\n    user_item_history = train_data.groupby('customer_id')['product_id'].apply(list).to_dict()\n\n    # Pad sequences for each user\n    padded_user_item_history = pad_sequences(user_item_history, num_items, product_id_mapping, device=device)\n\n    recommendations = []\n\n    for user_id in range(num_users):\n        # Get padded item sequence for the user\n        item_seq = padded_user_item_history.get(user_id, torch.zeros(num_items, dtype=torch.long).to(device))\n\n        # Prepare user IDs\n        user_ids = torch.tensor([user_id] * num_items, dtype=torch.long).to(device)\n\n        # Pass item features into the model\n        features = item_features.unsqueeze(0).to(device)  # Add batch dimension\n        scores = model(user_ids, item_seq.unsqueeze(0), features).detach().cpu().numpy().flatten()\n\n        # Get top 10 recommendations\n        top_10_item_indices = scores.argsort()[-10:][::-1]\n\n        for rank, item_idx in enumerate(top_10_item_indices, start=1):\n            recommendations.append({\n                'customer_id': user_id,\n                'product_id': item_idx,\n                'rank': rank\n            })\n\n    return pd.DataFrame(recommendations)\n\nproduct_features = (\n    data_train.drop_duplicates(subset='product_id')  # Keep one row per product_id\n    .set_index('product_id')  # Use product_id as the index\n    .select_dtypes(include=['number'])  # Select only numeric columns for features\n    .to_numpy()\n)\nitem_features = torch.tensor(product_features, dtype=torch.float)\n\ntop_10_recommendations = generate_top_10_recommendations(model, train_data, item_features, num_users, num_items, product_id_mapping, device='cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T21:52:34.188059Z","iopub.execute_input":"2025-01-11T21:52:34.188845Z","iopub.status.idle":"2025-01-11T22:01:32.618439Z","shell.execute_reply.started":"2025-01-11T21:52:34.188770Z","shell.execute_reply":"2025-01-11T22:01:32.617242Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def hitrate_at_k(true_data: pd.DataFrame, predicted_data: pd.DataFrame, k: int = 10) -> float:\n    \"\"\"\n    This function calculates the hitrate at k for the recommendations.\n    It assesses how relevant our 10 product recommendations are.\n    Args:\n        true_data: True product purchase data\n        predicted_data: Predicted product recommendations\n        k: The number of top recommendations to consider\n    Returns:\n        The hitrate at k\n    \"\"\"\n    # Ensure customer_id columns are strings before extracting numeric part (if needed)\n    true_data['customer_id'] = true_data['customer_id'].astype(str)\n    predicted_data['customer_id'] = predicted_data['customer_id'].astype(str)\n\n    # Option 1: Extract numeric customer_id from strings if needed\n    true_data['customer_id'] = true_data['customer_id'].str.extract('(\\d+)', expand=False).astype(int)\n    predicted_data['customer_id'] = predicted_data['customer_id'].str.extract('(\\d+)', expand=False).astype(int)\n\n    # Ensure product_id columns are strings before extracting numeric part\n    true_data['product_id'] = true_data['product_id'].astype(str)\n    predicted_data['product_id'] = predicted_data['product_id'].astype(str)\n\n    # Extract numeric part from product_id and convert to integers\n    true_data['product_id'] = true_data['product_id'].str.extract('(\\d+)', expand=False).astype(int)\n    predicted_data['product_id'] = predicted_data['product_id'].str.extract('(\\d+)', expand=False).astype(int)\n\n    # Merge the true and predicted data on customer_id and product_id\n    data = pd.merge(left=true_data, right=predicted_data, how=\"left\", on=[\"customer_id\", \"product_id\"])\n\n    # Filter recommendations where rank <= k\n    df = data[data[\"rank\"] <= k]\n\n    # Calculate the number of successful recommendations per user\n    non_null_counts = df.groupby('customer_id')['rank'].apply(lambda x: x.notna().sum()).reset_index(name='non_null_count')\n\n    # Calculate the hitrate as the ratio of successful recommendations\n    total_users = len(true_data['customer_id'].unique())\n    hitrate = non_null_counts['non_null_count'].sum() / (total_users * k)\n\n    return hitrate\n\n# Calculate hitrate@10 with the updated function\nhitrate_at_10 = hitrate_at_k(test_data, top_10_recommendations, k=10)\nprint(f\"Hitrate@10 for the model is {hitrate_at_10:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T00:19:54.317836Z","iopub.execute_input":"2025-01-12T00:19:54.318406Z","iopub.status.idle":"2025-01-12T00:19:54.324181Z","shell.execute_reply.started":"2025-01-12T00:19:54.318369Z","shell.execute_reply":"2025-01-12T00:19:54.322978Z"}},"outputs":[{"name":"stdout","text":"Hitrate@10 for the model is 10.02\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"user_item_history = train_data.groupby('customer_id')['product_id'].apply(list).to_dict()\n\ndef create_product_id_mapping(user_item_history):\n    \"\"\"\n    Create a mapping from product_id (string) to an integer.\n    \"\"\"\n    unique_product_ids = set([product_id for seq in user_item_history.values() for product_id in seq])\n    product_id_mapping = {product_id: idx for idx, product_id in enumerate(unique_product_ids)}\n    return product_id_mapping\n\ndef pad_sequences(user_item_history, num_items, product_id_mapping, device='cpu'):\n    # Pad sequences to a minimum length and convert product_ids to integers\n    padded_sequences = {}\n    for user_id, seq in user_item_history.items():\n        # Convert product IDs to integers using the mapping\n        seq = [product_id_mapping.get(product_id, -1) for product_id in seq]  # -1 for unknown product_ids\n\n        if len(seq) < num_items:  # Pad if sequence length is smaller\n            pad_length = num_items - len(seq)\n            seq = seq + [0] * pad_length  # Padding with 0\n        padded_sequences[user_id] = torch.tensor(seq, dtype=torch.long).to(device)\n\n    return padded_sequences\n\n# Create product ID mapping\nproduct_id_mapping = create_product_id_mapping(user_item_history)\n\n# Apply padding to user-item history with the mapping\npadded_user_item_history = pad_sequences(user_item_history, num_items, product_id_mapping, device='cpu')\n\ndef generate_top_10_recommendations(model, train_data, item_features, num_users, num_items, product_id_mapping, device='cpu'):\n    \"\"\"\n    Generate top 10 recommendations for each user from the trained Caser model.\n\n    Args:\n        model: Trained Caser model.\n        train_data: DataFrame containing historical transaction data.\n        item_features: Tensor containing product features.\n        num_users: Total number of users in the dataset.\n        num_items: Total number of items in the dataset.\n        product_id_mapping: Mapping from product_id (str) to integer index.\n        device: Device to run the model on (e.g., 'cpu' or 'cuda').\n\n    Returns:\n        DataFrame with columns: 'customer_id', 'product_id', 'rank'.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n\n    # Create user-item history from transaction data\n    user_item_history = train_data.groupby('customer_id')['product_id'].apply(list).to_dict()\n\n    # Pad sequences for each user\n    padded_user_item_history = pad_sequences(user_item_history, num_items, product_id_mapping, device=device)\n\n    recommendations = []\n\n    for user_id in range(num_users):\n        # Get padded item sequence for the user\n        item_seq = padded_user_item_history.get(user_id, torch.zeros(num_items, dtype=torch.long).to(device))\n\n        # Prepare user IDs\n        user_ids = torch.tensor([user_id] * num_items, dtype=torch.long).to(device)\n\n        # Pass item features into the model\n        features = item_features.unsqueeze(0).to(device)  # Add batch dimension\n        scores = model(user_ids, item_seq.unsqueeze(0), features).detach().cpu().numpy().flatten()\n        \n        for item_idx, score in enumerate(scores):\n            recommendations.append({\n                'customer_id': user_id,\n                'product_id': item_idx,\n                'score': scores\n            })\n\n    return pd.DataFrame(recommendations)\n\nproduct_features = (\n    data_train.drop_duplicates(subset='product_id')  # Keep one row per product_id\n    .set_index('product_id')  # Use product_id as the index\n    .select_dtypes(include=['number'])  # Select only numeric columns for features\n    .to_numpy()\n)\nitem_features = torch.tensor(product_features, dtype=torch.float)\n\ntop_10_recommendations = generate_top_10_recommendations(model, train_data, item_features, num_users, num_items, product_id_mapping, device='cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T22:01:35.427568Z","iopub.execute_input":"2025-01-11T22:01:35.428015Z","execution_failed":"2025-01-11T22:04:43.578Z"}},"outputs":[],"execution_count":null}]}